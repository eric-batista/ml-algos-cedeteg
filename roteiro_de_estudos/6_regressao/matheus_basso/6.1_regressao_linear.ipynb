{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af4d9ce",
   "metadata": {},
   "source": [
    "# 6.1 Regressão Linear\n",
    "\n",
    "A regressão linear é talvez um dos algoritmos mais conhecidos e compreendidos em estatística e aprendizado de máquina. A regressão linear foi desenvolvida no campo da estatística e é estudada como um modelo para entender a relação entre variáveis numéricas de entrada e saída, mas com o passar do tempo, tornou-se parte integrante da caixa de ferramentas de aprendizado de máquina moderna.\n",
    "\n",
    "A regressão pertence à classe de tarefas de Aprendizado Supervisionado em que os conjuntos de dados usados para modelagem preditiva / estatística contêm rótulos contínuos. Mas, vamos definir um problema de regressão mais matematicamente.\n",
    "\n",
    "Vamos considerar a seguinte imagem abaixo:\n",
    "\n",
    "![fig_2](https://image.ibb.co/jRH8E9/Capture.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5b07b",
   "metadata": {},
   "source": [
    "Assim, na imagem acima, X é o conjunto de valores que correspondem às áreas de convivência de várias casas (também considerado como o espaço de valores de entrada) ey é o preço das respectivas casas, mas observe que esses valores são previstos por h . h é a função que mapeia os valores de X para y (geralmente chamado de preditor). Por razões históricas, este h é referido como uma função de hipótese. Lembre-se de que este conjunto de dados apresentou apenas, ou seja, as áreas de convivência de várias casas e considere-o como um conjunto de dados de brinquedo para fins de compreensão.\n",
    "\n",
    "Observe que os valores previstos aqui são contínuos por natureza. Portanto, seu objetivo final é, dado um conjunto de treinamento, aprender uma função h: X → Y de modo que h (x) seja um \"bom\" preditor para o valor correspondente de y. Além disso, tenha em mente que o domínio de valores que X e Y aceitam são todos números reais e você pode defini-lo assim: X = Y = $\\mathbb{R}$ onde, $\\mathbb{R}$ é o conjunto de todos os números reais.\n",
    "\n",
    "Um par (x (i), y (i)) é chamado de exemplo de treinamento. Você pode definir o conjunto de treinamento como {(x (i), y (i)); i = 1, ..., m} (caso o conjunto de treinamento contenha m instâncias e haja apenas um recurso x no conjunto de dados).\n",
    "\n",
    "Um pouco de matemática aí para você, para que não se engane nem nas coisas mais simples. Então, de acordo com Han, Kamber e Pei-\n",
    "\n",
    "\"Em geral, esses métodos são usados para prever o valor de uma variável de resposta (dependente) de uma ou mais variáveis preditoras (independentes), onde as variáveis são numéricas.\" - Data Mining: Concepts and Techniques (3ª ed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c76fdb",
   "metadata": {},
   "source": [
    "Para realizar a regressão, você deve decidir a maneira como representará h. Como escolha inicial, digamos que você decida aproximar y como uma função linear de x:\n",
    "\n",
    "$h_\\theta (x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2$\n",
    "\n",
    "Aqui, os $θ_i$ são os parâmetros (também chamados de pesos) que parametrizam o espaço de funções lineares mapeando de X a Y. Em um sentido mais simples, esses parâmetros são usados para mapear com precisão X a Y. Mas para manter as coisas simples para sua compreensão, você eliminará o subscrito $\\theta$ em $h_\\theta (x)$ e o escreverá simplesmente como $h(x)$. Para simplificar ainda mais a sua notação, você também introduzirá a convenção de deixar $x_\\theta = 1$ (este é o termo de interceptação), de modo que\n",
    "\n",
    "$$h(x) = \\sum^{n}_{i=o}\\theta_ix_i=\\theta^Tx,$$\n",
    "\n",
    "onde no lado direito acima você está considerando θ e x ambos como vetores, e aqui n é o número de instâncias de entrada (sem contar x$\\theta$).\n",
    "\n",
    "Mas a principal questão que se levanta neste ponto é como você escolhe ou aprende os parâmetros $\\theta$? Você não pode alterar suas instâncias de entrada para prever os preços. Você tem apenas esses parâmetros θ para sintonizar / ajustar.\n",
    "\n",
    "Um método proeminente parece ser fazer h (x) próximo de y, pelo menos para os exemplos de treinamento que você tem. Para entender isso mais formalmente, vamos tentar definir uma função que determina, para cada valor de $\\theta$'s, quão próximos os $h(x^(i))$'s estão dos y (i)' s correspondentes. A função deve ser semelhante a esta:\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{2}\\sum^{m}_{i=1}(h_\\theta(x^{(i)})-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c7be72",
   "metadata": {},
   "source": [
    "Você acabou de ver uma das fórmulas mais importantes do mundo da Ciência de Dados / Aprendizado de Máquina / Estatística. É chamado de função de custo.\n",
    "\n",
    "Esta é uma derivação essencial porque não só dá origem à próxima evolução da regressão linear (Mínimos Quadrados Ordinários), mas também formula as bases de uma classe inteira de algoritmos de modelagem linear (lembre-se de que você encontrou um termo chamado Modelos Lineares Generalizados).\n",
    "\n",
    "É importante notar que a regressão linear pode muitas vezes ser dividida em duas formas básicas:\n",
    "\n",
    "* Regressão Linear Simples (SLR), que lida com apenas duas variáveis (aquela que você viu primeiro)\n",
    "* Regressão Multi-linear (MLR) que lida com mais de duas variáveis (a que você acabou de ver)\n",
    "\n",
    "Essas coisas são muito diretas, mas muitas vezes podem causar confusão.\n",
    "\n",
    "Você já lançou suas bases para a regressão linear. Agora você vai estudar mais sobre as maneiras de estimar os parâmetros que viu na seção acima. Essa estimativa de parâmetros é essencialmente conhecida como treinamento de regressão linear. Agora, existem muitos métodos para treinar um modelo de regressão linear Ordinary Least Squares (OLS), sendo o mais popular entre eles. Portanto, é bom referir um modelo de regressão linear treinado usando OLS como Regressão Linear de Mínimos Quadrados Ordinários ou apenas Regressão de Mínimos Quadrados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b6295",
   "metadata": {},
   "source": [
    " ***Observe que os parâmetros aqui neste contexto também são chamados de coeficientes do modelo.***\n",
    "\n",
    "## Otimizando um modelo de regressão linear - várias abordagens:\n",
    "\n",
    "Aprender / treinar um modelo de regressão linear significa essencialmente estimar os valores dos coeficientes / parâmetros usados na representação com os dados que você tem.\n",
    "\n",
    "Nesta seção, você dará uma breve olhada em algumas técnicas para preparar um modelo de regressão linear.\n",
    "\n",
    "## Regressão de mínimos quadrados:\n",
    "\n",
    "Você saiu da seção anterior com a noção de escolher $\\theta$ de forma a minimizar J ($\\theta$). Para fazer isso, vamos usar um algoritmo de pesquisa que começa com alguma \"estimativa inicial\" para θ, e que altera iterativamente θ para tornar J ($\\theta$) menor, até que, esperançosamente, você convirja para um valor de θ que minimiza J (θ). Especificamente, vamos considerar o algoritmo de descida de gradiente, que começa com algum θ inicial e executa repetidamente a atualização:\n",
    "\n",
    "$$\\theta_j:=\\theta_j-\\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)$$\n",
    "\n",
    "Intuitivamente falando, a fórmula acima denota a pequena mudança que acontece em J w.r.t o parâmetro $\\theta j$ e como isso afeta o valor inicial de $\\theta j$. Mas olhe com cuidado, você tem uma derivada parcial aqui para lidar. Todo o processo de derivação está fora do escopo deste tutorial.\n",
    "\n",
    "Observe que, para um único exemplo de treinamento, isso fornece a regra de atualização:\n",
    "\n",
    "$$\\theta_j:= \\theta_j+\\alpha(y^{(i)}-h_\\theta(x^{(i)})x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f19d0",
   "metadata": {},
   "source": [
    "A regra é chamada de regra de atualização de LMS (LMS significa “mínimos quadrados médios”) e também é conhecida como regra de aprendizado de Widrow-Hoff.\n",
    "\n",
    "Vamos resumir algumas coisas no contexto do OLS.\n",
    "\n",
    "\"O procedimento dos Mínimos Quadrados Ordinários visa minimizar a soma dos resíduos quadrados. Isso significa que, dada uma linha de regressão através dos dados, calculamos a distância de cada ponto de dados até a linha de regressão, elevamos ao quadrado e somamos todos os erros quadrados. . Esta é a quantidade que os mínimos quadrados comuns procuram minimizar. \" - Jason Brownlee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aae89e",
   "metadata": {},
   "source": [
    "### Otimização com gradiente descendente:\n",
    "\n",
    "Na regra de treinamento anterior, você já teve a noção de como a descida gradiente pode ser incorporada neste contexto. Essencialmente, a descida gradiente é um processo de otimização dos valores dos coeficientes, minimizando iterativamente o erro do modelo em seus dados de treinamento.\n",
    "\n",
    "Falando mais resumidamente, ele funciona começando com valores aleatórios para cada coeficiente. A soma dos erros quadrados é calculada para cada par de valores de entrada e saída. Uma taxa de aprendizagem é usada como um fator de escala e os coeficientes são atualizados no sentido de minimizar o erro. O processo é repetido até que um erro quadrático de soma mínima seja alcançado ou nenhuma melhoria adicional seja possível.\n",
    "\n",
    "O termo α (taxa de aprendizado) é muito importante aqui, pois determina o tamanho da etapa de melhoria a ser realizada em cada iteração do procedimento.\n",
    "\n",
    "Agora, existem normalmente duas variantes de descida gradiente:\n",
    "\n",
    "* O método que examina todos os exemplos em todo o conjunto de treinamento em cada etapa e é chamado de descida gradiente em lote.\n",
    "\n",
    "* O método em que você executa repetidamente o conjunto de treinamento e, cada vez que encontra um exemplo de treinamento, atualiza os parâmetros de acordo com o gradiente do erro em relação apenas a esse único exemplo de treinamento. Este algoritmo é denominado descida gradiente estocástica (também descida gradiente incremental).\n",
    "\n",
    "Isso é tudo para a descida gradiente para este tutorial. Agora, você dá uma olhada em outra maneira de otimizar um modelo de regressão linear, ou seja, regularização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629aea5",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eec33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "data = datasets.load_boston()\n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "target = pd.DataFrame(data.target, columns=[\"MEDV\"])\n",
    "X = df\n",
    "y = target[\"MEDV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3ec2f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "model = lm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a25225fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.00384338 25.02556238 30.56759672 28.60703649 27.94352423]\n"
     ]
    }
   ],
   "source": [
    "predictions = lm.predict(X)\n",
    "print(predictions[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1162312c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7406426641094095"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb36f557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.08011358e-01,  4.64204584e-02,  2.05586264e-02,  2.68673382e+00,\n",
       "       -1.77666112e+01,  3.80986521e+00,  6.92224640e-04, -1.47556685e+00,\n",
       "        3.06049479e-01, -1.23345939e-02, -9.52747232e-01,  9.31168327e-03,\n",
       "       -5.24758378e-01])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
